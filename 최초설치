🚀 1️⃣ Docker에서 Ubuntu 컨테이너 설치하기
🔹 1. Docker Desktop 실행 및 Ubuntu 이미지 다운로드
먼저, Docker Desktop을 실행한 후, Ubuntu 컨테이너를 실행하기 위해 공식 Ubuntu 이미지를 다운로드해야 해.

터미널(CMD 또는 PowerShell)에서 아래 명령어를 입력:

bash
복사
편집
docker pull ubuntu:22.04
👉 이 명령어는 Ubuntu 22.04 LTS 이미지를 다운로드한다.

🔹 2. Ubuntu 컨테이너 실행 및 생성
이제 Ubuntu 컨테이너를 실행하자. 다음 명령어를 입력:
docker run -it --name su_ubuntu_container --gpus all --shm-size=32g --memory=50g --cpus=16 ubuntu:22.04 /bin/bash


🚀 2️⃣ 컨테이너에서 필수 패키지 설치
Ubuntu 컨테이너 내부에서 Ollama 및 Deepseek R1을 설치하기 위해 기본 패키지를 먼저 설치해야 해.

🔹 1. 패키지 목록 업데이트
bash
복사
편집

apt update && apt upgrade -y
🔹 2. 필수 패키지 설치
bash
복사
편집
apt install -y curl git wget nano build-essential
📌 설명:

curl → Ollama 설치에 필요
git → 코드 관리용
wget → 다운로드 도구
nano → 간단한 텍스트 편집기
build-essential → C++ 컴파일러 및 필수 빌드 도구

🚀 3️⃣ 컨테이너를 백그라운드에서 실행하기
지금까지 실행한 Ubuntu 컨테이너는 종료하면 사라지므로, 백그라운드에서 실행되도록 설정해보자.

🔹 1. 현재 컨테이너에서 나가기
컨테이너 내부에서 exit 입력하거나 Ctrl + D를 누르면 빠져나올 수 있어.

🔹 2. 컨테이너를 백그라운드에서 실행
bash
복사
편집
docker start -ai su_ubuntu_container
👉 이 명령어를 사용하면 컨테이너를 계속 유지하면서 다시 접속 가능!


👉 이 명령어를 사용하면 컨테이너를 계속 유지하면서 다시 접속 가능!

또는 완전히 백그라운드에서 실행하려면:

bash
복사
편집
docker start su_ubuntu_container
docker exec -it su_
ubuntu_container /bin/bash









1️⃣ PowerShell에서 할 일 (관리자 권한)

powershell
복사
편집
wsl --shutdown
👉 이 부분은 PowerShell에서 실행하면 OK! 이미 실행했으니 넘어가도 돼.

2️⃣ Ubuntu 컨테이너 내부에서 실행해야 할 명령어 PowerShell이 아니라, Ubuntu 컨테이너 내부에서 실행해야 해.

🔹 Ubuntu 컨테이너 내부로 들어가기
PowerShell에서 다음 명령어 입력:

powershell
복사
편집
docker exec -it ubuntu_container /bin/bash
👉 이 명령어를 실행하면 Ubuntu 컨테이너 내부로 진입할 수 있어.






PS C:\Users\fruup> docker exec -it su_ubuntu_container /bin/bash
root@a7cd8bb44e21:/#


이후부터는 어떻게 진행해야 해?
ChatGPT의 말:
이제 Docker에서 실행 중인 Ubuntu 컨테이너 내부에 들어왔습니다.
이제 Ollama 및 Deepseek R1 14B를 설치하고, RTX 4070을 활용한 학습 환경을 구성하는 작업을 진행합니다.

🚀 1️⃣ Ubuntu 컨테이너 기본 설정
컨테이너 내부에서 패키지를 업데이트하고 기본 도구들을 설치합니다.

bash
복사
편집
apt update && apt upgrade -y
apt install -y curl git python3 python3-pip nano
✅ Python 및 패키지 관리 도구가 설치됩니다.

🚀 2️⃣ NVIDIA GPU 드라이버 및 CUDA 설정
RTX 4070을 학습에 활용하려면 CUDA 및 NVIDIA 관련 라이브러리를 설치해야 합니다.

(1) NVIDIA 드라이버 확인
bash
복사
편집
nvidia-smi
✅ 정상적인 경우 다음과 같은 GPU 정보가 출력되어야 합니다.

pgsql
복사
편집
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 530.41.03    Driver Version: 530.41.03    CUDA Version: 12.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
+-------------------------------+----------------------+----------------------+







🚀 3️⃣ Ollama 설치
이제 Ollama를 설치하여 Deepseek R1 14B 모델을 실행할 수 있도록 합니다.

bash
복사
편집
curl -fsSL https://ollama.com/install.sh | sh
✅ 설치가 완료되면, 버전을 확인하세요.

bash
복사
편집
ollama --version


curl http://127.0.0.1:11434/api/tags 명령어의 응답이:

json
복사
편집
{"models":[]}
즉, 설치된 모델이 없음을 의미합니다.

💡 Ollama 서버는 정상 작동 중이지만, Deepseek R1 모델이 아직 다운로드되지 않았습니다.

🚀 다음 단계: Deepseek R1 14B 모델 다운로드
이제 Deepseek R1 14B 모델을 다운로드해서 실행할 수 있도록 설정해야 합니다.

1️⃣ Deepseek R1 14B 모델 다운로드
아래 명령어를 실행하여 Deepseek R1 14B 모델을 다운로드하세요.

bash
복사
편집
ollama pull deepseek-r1:14b
이 명령어를 실행하면 Deepseek R1 14B 모델을 Ollama에 다운로드하여 사용 가능하게 합니다.
💡 다운로드 속도는 네트워크 상태에 따라 다를 수 있으며, 모델 크기가 크므로 시간이 걸릴 수 있습니다.


2️⃣ 모델 다운로드 확인
다운로드가 완료되었는지 확인하려면 다시 다음 명령어를 실행합니다.

bash
복사
편집
curl http://127.0.0.1:11434/api/tags
🔹 응답이 다음과 같이 나오면 성공적으로 모델이 설치된 것입니다.

json
복사
편집
{"models":["deepseek-r1:14b"]}
설치된 모델 리스트에 **deepseek-r1:14b**가 포함되어 있어야 합니다.


















🚀 Deepseek R1을 WebUI(Open WebUI)와 연동하여 브라우저에서 실행하는 방법
지금까지 Ollama를 Ubuntu 컨테이너(su_ubuntu_container) 내부에 설치했으므로,
WebUI(Open WebUI)를 새로운 컨테이너에서 실행하여 Ollama와 연동해야 해.

1️⃣ WebUI(Open WebUI) 실행을 위한 준비 사항
✅ Ollama는 su_ubuntu_container 컨테이너 내부에 설치됨
✅ WebUI는 별도의 Docker 컨테이너로 실행하여 Ollama와 연동할 것
✅ Windows에서 브라우저(http://localhost:3000)로 접속 가능하게 설정

2️⃣ WebUI(Open WebUI) 컨테이너 실행
PowerShell을 관리자 권한으로 열고 아래 명령어 실행:

powershell
복사
편집
docker run -d --name ollama-webui --gpus all -p 3000:3000 -v ~/.ollama:/root/.ollama ghcr.io/open-webui/open-webui:main
📌 설명

-d → 백그라운드에서 실행
--name ollama-webui → 컨테이너 이름 ollama-webui로 설정
--gpus all → GPU 사용 가능하게 설정
-p 3000:3000 → 호스트(Windows)와 컨테이너 포트(3000) 연결
-v ~/.ollama:/root/.ollama → Ollama의 모델 데이터를 공유
ghcr.io/open-webui/open-webui:main → Open WebUI 최신 버전 실행
✅ 실행 후 브라우저에서 http://localhost:3000 접속하면 WebUI가 열림.































