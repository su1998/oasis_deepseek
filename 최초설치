🚀 1️⃣ Docker에서 Ubuntu 컨테이너 설치하기
🔹 1. Docker Desktop 실행 및 Ubuntu 이미지 다운로드
먼저, Docker Desktop을 실행한 후, Ubuntu 컨테이너를 실행하기 위해 공식 Ubuntu 이미지를 다운로드해야 해.

터미널(CMD 또는 PowerShell)에서 아래 명령어를 입력:

bash
복사
편집
docker pull ubuntu:22.04
👉 이 명령어는 Ubuntu 22.04 LTS 이미지를 다운로드한다.

🔹 2. Ubuntu 컨테이너 실행 및 생성
이제 Ubuntu 컨테이너를 실행하자. 다음 명령어를 입력:
docker run -it --name su_ubuntu_container --gpus all --shm-size=32g --memory=50g --cpus=16 ubuntu:22.04 /bin/bash


🚀 2️⃣ 컨테이너에서 필수 패키지 설치
Ubuntu 컨테이너 내부에서 Ollama 및 Deepseek R1을 설치하기 위해 기본 패키지를 먼저 설치해야 해.

🔹 1. 패키지 목록 업데이트
bash
복사
편집

apt update && apt upgrade -y
🔹 2. 필수 패키지 설치
bash
복사
편집
apt install -y curl git wget nano build-essential
📌 설명:

curl → Ollama 설치에 필요
git → 코드 관리용
wget → 다운로드 도구
nano → 간단한 텍스트 편집기
build-essential → C++ 컴파일러 및 필수 빌드 도구

🚀 3️⃣ 컨테이너를 백그라운드에서 실행하기
지금까지 실행한 Ubuntu 컨테이너는 종료하면 사라지므로, 백그라운드에서 실행되도록 설정해보자.

🔹 1. 현재 컨테이너에서 나가기
컨테이너 내부에서 exit 입력하거나 Ctrl + D를 누르면 빠져나올 수 있어.

🔹 2. 컨테이너를 백그라운드에서 실행
bash
복사
편집
docker start -ai su_ubuntu_container
👉 이 명령어를 사용하면 컨테이너를 계속 유지하면서 다시 접속 가능!


👉 이 명령어를 사용하면 컨테이너를 계속 유지하면서 다시 접속 가능!

또는 완전히 백그라운드에서 실행하려면:

bash
복사
편집
docker start su_ubuntu_container
docker exec -it su_
ubuntu_container /bin/bash









1️⃣ PowerShell에서 할 일 (관리자 권한)

powershell
복사
편집
wsl --shutdown
👉 이 부분은 PowerShell에서 실행하면 OK! 이미 실행했으니 넘어가도 돼.

2️⃣ Ubuntu 컨테이너 내부에서 실행해야 할 명령어 PowerShell이 아니라, Ubuntu 컨테이너 내부에서 실행해야 해.

🔹 Ubuntu 컨테이너 내부로 들어가기
PowerShell에서 다음 명령어 입력:

powershell
복사
편집
docker exec -it ubuntu_container /bin/bash
👉 이 명령어를 실행하면 Ubuntu 컨테이너 내부로 진입할 수 있어.






PS C:\Users\fruup> docker exec -it su_ubuntu_container /bin/bash
root@a7cd8bb44e21:/#


이후부터는 어떻게 진행해야 해?
ChatGPT의 말:
이제 Docker에서 실행 중인 Ubuntu 컨테이너 내부에 들어왔습니다.
이제 Ollama 및 Deepseek R1 14B를 설치하고, RTX 4070을 활용한 학습 환경을 구성하는 작업을 진행합니다.

🚀 1️⃣ Ubuntu 컨테이너 기본 설정
컨테이너 내부에서 패키지를 업데이트하고 기본 도구들을 설치합니다.

bash
복사
편집
apt update && apt upgrade -y
apt install -y curl git python3 python3-pip nano
✅ Python 및 패키지 관리 도구가 설치됩니다.

🚀 2️⃣ NVIDIA GPU 드라이버 및 CUDA 설정
RTX 4070을 학습에 활용하려면 CUDA 및 NVIDIA 관련 라이브러리를 설치해야 합니다.

(1) NVIDIA 드라이버 확인
bash
복사
편집
nvidia-smi
✅ 정상적인 경우 다음과 같은 GPU 정보가 출력되어야 합니다.

pgsql
복사
편집
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 530.41.03    Driver Version: 530.41.03    CUDA Version: 12.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
+-------------------------------+----------------------+----------------------+







🚀 3️⃣ Ollama 설치
이제 Ollama를 설치하여 Deepseek R1 14B 모델을 실행할 수 있도록 합니다.

bash
복사
편집
curl -fsSL https://ollama.com/install.sh | sh
✅ 설치가 완료되면, 버전을 확인하세요.

bash
복사
편집
ollama --version


curl http://127.0.0.1:11434/api/tags 명령어의 응답이:

json
복사
편집
{"models":[]}
즉, 설치된 모델이 없음을 의미합니다.

💡 Ollama 서버는 정상 작동 중이지만, Deepseek R1 모델이 아직 다운로드되지 않았습니다.

🚀 다음 단계: Deepseek R1 14B 모델 다운로드
이제 Deepseek R1 14B 모델을 다운로드해서 실행할 수 있도록 설정해야 합니다.

1️⃣ Deepseek R1 14B 모델 다운로드
아래 명령어를 실행하여 Deepseek R1 14B 모델을 다운로드하세요.

bash
복사
편집
ollama pull deepseek-r1:14b
이 명령어를 실행하면 Deepseek R1 14B 모델을 Ollama에 다운로드하여 사용 가능하게 합니다.
💡 다운로드 속도는 네트워크 상태에 따라 다를 수 있으며, 모델 크기가 크므로 시간이 걸릴 수 있습니다.


2️⃣ 모델 다운로드 확인
다운로드가 완료되었는지 확인하려면 다시 다음 명령어를 실행합니다.

bash
복사
편집
curl http://127.0.0.1:11434/api/tags
🔹 응답이 다음과 같이 나오면 성공적으로 모델이 설치된 것입니다.

json
복사
편집
{"models":["deepseek-r1:14b"]}
설치된 모델 리스트에 **deepseek-r1:14b**가 포함되어 있어야 합니다.


















🚀 Deepseek R1을 WebUI(Open WebUI)와 연동하여 브라우저에서 실행하는 방법
지금까지 Ollama를 Ubuntu 컨테이너(su_ubuntu_container) 내부에 설치했으므로,
WebUI(Open WebUI)를 새로운 컨테이너에서 실행하여 Ollama와 연동해야 해.

1️⃣ WebUI(Open WebUI) 실행을 위한 준비 사항
✅ Ollama는 su_ubuntu_container 컨테이너 내부에 설치됨
✅ WebUI는 별도의 Docker 컨테이너로 실행하여 Ollama와 연동할 것
✅ Windows에서 브라우저(http://localhost:3000)로 접속 가능하게 설정

2️⃣ WebUI(Open WebUI) 컨테이너 실행
PowerShell을 관리자 권한으로 열고 아래 명령어 실행:

powershell
복사
편집
docker run -d --name ollama-webui --gpus all -p 3000:3000 -v ~/.ollama:/root/.ollama ghcr.io/open-webui/open-webui:main
📌 설명

-d → 백그라운드에서 실행
--name ollama-webui → 컨테이너 이름 ollama-webui로 설정
--gpus all → GPU 사용 가능하게 설정
-p 3000:3000 → 호스트(Windows)와 컨테이너 포트(3000) 연결
-v ~/.ollama:/root/.ollama → Ollama의 모델 데이터를 공유
ghcr.io/open-webui/open-webui:main → Open WebUI 최신 버전 실행
✅ 실행 후 브라우저에서 http://localhost:3000 접속하면 WebUI가 열림.


PS C:\Users\fruup> docker exec -it ollama-webui /bin/bash
root@6217353f5426:/app/backend# curl http://host.docker.internal:11434/api/tags
{"models":[{"name":"deepseek-r1:14b","model":"deepseek-r1:14b","modified_at":"2025-02-10T20:05:39.864969083+09:00","size":8988112040,"digest":"ea35dfe18182f635ee2b214ea30b7520fe1ada68da018f8b395b444b662d4f1a","details":{"parent_model":"","format":"gguf","family":"qwen2","families":["qwen2"],"parameter_size":"14.8B","quantization_level":"Q4_K_M"}}]}root@6217353f5426:/app/backend#
















🔹 WebUI 컨테이너 내부로 접속

powershell
복사
편집
docker exec -it ollama-webui /bin/bash





***************************************


1️⃣ 실행 중인 컨테이너 목록 확인
현재 실행 중인 컨테이너만 확인하려면 다음 명령어를 사용하면 돼.

powershell
복사
편집
docker ps


Ubuntu 컨테이너(su_ubuntu_container) 내부 접속
powershell
복사
편집
docker exec -it su_ubuntu_container /bin/bash

🚀 Deepseek R1 실행 방법 (ollama pull deepseek-r1:14b 설치 완료 후)
현재 deepseek-r1:14b 모델을 Ubuntu 컨테이너(su_ubuntu_container) 내부에 설치했으므로, 모델을 실행하는 방법을 알려줄게.

1️⃣ Ollama 서버 실행 (ollama serve &)
먼저, Ollama 서버를 백그라운드에서 실행해야 함.
컨테이너 내부에서 아래 명령어를 입력하자.

bash
복사
편집



ollama serve &



✅ 실행 후 Ollama API가 127.0.0.1:11434에서 실행됨.

2️⃣ Deepseek R1 모델 실행 테스트
이제 설치한 deepseek-r1:14b 모델을 실행해보자.

bash
복사
편집
ollama run deepseek-r1:14b
✅ 실행 후 프롬프트가 뜨면, 여기에 질문을 입력하면 모델이 응답을 생성함.

3️⃣ Deepseek R1 모델 실행 (대화 모드)
만약 채팅 방식으로 모델을 사용하고 싶다면, 다음 명령어를 실행하면 돼.

bash
복사
편집
ollama chat deepseek-r1:14b
이제 프롬프트에서 대화형으로 질문을 입력하고 모델과 소통할 수 있음!



---------------------------> 순서정리

파워셀에서
docker exec -it su_ubuntu_container /bin/bash

먼저, Ollama 서버를 백그라운드에서 실행해야 함.
컨테이너 내부에서 아래 명령어를 입력하자.

ollama serve &

질문
ollama run deepseek-r1:14b

대화형
ollama chat deepseek-r1:14b




______________________________________>>>>>>>
WebUI 컨테이너(ollama-webui) 내부 접속

powershell
복사
편집
docker exec -it ollama-webui /bin/bash

docker exec -it ollama-webui /bin/bash































