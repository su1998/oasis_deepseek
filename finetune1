import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from datasets import load_dataset

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (FP8 ì–‘ìí™” ì œê±°, GPU ìµœì í™” ì ìš©)
model_name = "deepseek-ai/deepseek-r1"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    trust_remote_code=True,
    torch_dtype=torch.float16,  # ğŸš€ FP16 ì ìš© (ë©”ëª¨ë¦¬ ìµœì í™”)
    device_map="auto"  # ğŸš€ GPU ìë™ í• ë‹¹
)

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

# ë°ì´í„°ì…‹ ë¡œë“œ (JSONL íŒŒì¼ ë¡œë“œ)
dataset = load_dataset("json", data_files="dataset.jsonl")

# Fine-Tuning ì„¤ì • (ë©”ëª¨ë¦¬ ìµœì í™” ì ìš©)
training_args = TrainingArguments(
    output_dir="./fine_tuned_model",  # ëª¨ë¸ ì €ì¥ ê²½ë¡œ
    per_device_train_batch_size=1,  # ğŸš€ ë©”ëª¨ë¦¬ ì ˆì•½ (ìµœì ê°’: 1 ë˜ëŠ” 2)
    gradient_accumulation_steps=8,  # ğŸš€ ë°°ì¹˜ í¬ê¸°ë¥¼ ê°€ìƒì ìœ¼ë¡œ ëŠ˜ë ¤ ë©”ëª¨ë¦¬ ì ˆì•½
    fp16=True,  # ğŸš€ FP16 í™œìš© (ë©”ëª¨ë¦¬ ì ˆì•½ + ì†ë„ í–¥ìƒ)
    optim="adamw_bnb_8bit",  # ğŸš€ 8bit Adam ì˜µí‹°ë§ˆì´ì € ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
    num_train_epochs=3,  # í•™ìŠµ ì—í¬í¬ ìˆ˜
    save_steps=500,  # ì €ì¥ ë¹ˆë„
    save_total_limit=2,  # ìµœëŒ€ ì €ì¥ ëª¨ë¸ ê°œìˆ˜
    logging_dir="./logs",  # ë¡œê·¸ ì €ì¥ ê²½ë¡œ
    logging_steps=50,  # ë¡œê·¸ ì¶œë ¥ ë¹ˆë„
    evaluation_strategy="no",  # í‰ê°€ ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
    report_to="none",  # WandB ê°™ì€ ë¡œê¹… íˆ´ ì‚¬ìš© ì•ˆ í•¨
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    tokenizer=tokenizer,
)

# í•™ìŠµ ì‹œì‘
trainer.train()

# ëª¨ë¸ ì €ì¥
trainer.save_model("./fine_tuned_model")
tokenizer.save_pretrained("./fine_tuned_model")
